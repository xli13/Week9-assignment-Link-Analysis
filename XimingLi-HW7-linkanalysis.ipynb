{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7 Link analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The url I choose is https://doc.scrapy.org/en/latest/intro/tutorial.html. \n",
    "I wrote a crawler to grab first 200 URLs. And this crawler considers the complications in '<a href'. More details will be commented in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 Create a crawler using the webcrawler provided and discussed in my jupyter notebook ( https://github.com/pschragger/big-data-python-class/blob/master/Lectures/Lecture_7_-_Link_Analysis.ipynb) , have it crawl the first 200 pages from a base URL of your choosin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add url x to the list, updata matrix while crawling \n",
    "#fidx is the index of source url in urllist\n",
    "#idx is the index of of next free page url in urllist\n",
    "def addtolist(fidx,idx,x,url,matrix):\n",
    "    i=x.rfind('#')  # remove the last '#something...', because it's just pointing to some section in the html page\n",
    "    if i!=-1:\n",
    "        x=x[:i]  # get rid of '#'\n",
    "    if x not in urllist:  # a new page\n",
    "        urllist.append(x)\n",
    "        matrix[fidx][idx]=1\n",
    "        idx+=1\n",
    "    else:   # a old page\n",
    "        k=urllist.index(x) # find the index of old page\n",
    "        matrix[fidx][k]=1\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl from url and update matrix if needed\n",
    "#fidx is the index of url in urllist\n",
    "\n",
    "def crawl(fidx,url,urllist,matrix):\n",
    "    idx=len(urllist)\n",
    "    i=url.rfind('/')\n",
    "    me=url[0:i+1]  #current level \n",
    "    i=url.rfind('/',0,i-1)\n",
    "    parent=url[0:i+1] # get the upper level\n",
    "    i=url.find('://')\n",
    "    r=requests.get(url)\n",
    "    response= TextResponse(r.url, body=r.text, encoding='utf-8')\n",
    "    link=response.xpath('//ul/li/a/@href').extract()  # all url link\n",
    "    for x in link:\n",
    "        if x.endswith('://'): # get rid of '://' illeagal format\n",
    "            continue\n",
    "        elif x.startswith('#'): # point to url itself, but same page\n",
    "            matrix[fidx][fidx]=1\n",
    "        elif x.startswith('http'):\n",
    "            idx=addtolist(fidx,idx,x,urllist,matrix)\n",
    "            if idx==200:\n",
    "                return idx\n",
    "        elif x.startswith('../'):\n",
    "            x=parent+x[3:]\n",
    "            idx=addtolist(fidx,idx,x,urllist, matrix)\n",
    "            if idx==200:\n",
    "                return idx\n",
    "        else:\n",
    "            x=me+x\n",
    "            idx=addtolist(fidx,idx,x,urllist,matrix)\n",
    "            if idx==200:\n",
    "                return idx\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 200 URLs are:\n",
      "https://doc.scrapy.org/en/latest/intro/tutorial.html\n",
      "https://doc.scrapy.org/en/latest/intro/overview.html\n",
      "https://doc.scrapy.org/en/latest/intro/install.html\n",
      "https://doc.scrapy.org/en/latest/intro/examples.html\n",
      "https://doc.scrapy.org/en/latest/topics/commands.html\n",
      "https://doc.scrapy.org/en/latest/topics/spiders.html\n",
      "https://doc.scrapy.org/en/latest/topics/selectors.html\n",
      "https://doc.scrapy.org/en/latest/topics/items.html\n",
      "https://doc.scrapy.org/en/latest/topics/loaders.html\n",
      "https://doc.scrapy.org/en/latest/topics/shell.html\n",
      "https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
      "https://doc.scrapy.org/en/latest/topics/feed-exports.html\n",
      "https://doc.scrapy.org/en/latest/topics/request-response.html\n",
      "https://doc.scrapy.org/en/latest/topics/link-extractors.html\n",
      "https://doc.scrapy.org/en/latest/topics/settings.html\n",
      "https://doc.scrapy.org/en/latest/topics/exceptions.html\n",
      "https://doc.scrapy.org/en/latest/topics/logging.html\n",
      "https://doc.scrapy.org/en/latest/topics/stats.html\n",
      "https://doc.scrapy.org/en/latest/topics/email.html\n",
      "https://doc.scrapy.org/en/latest/topics/telnetconsole.html\n",
      "https://doc.scrapy.org/en/latest/topics/webservice.html\n",
      "https://doc.scrapy.org/en/latest/faq.html\n",
      "https://doc.scrapy.org/en/latest/topics/debug.html\n",
      "https://doc.scrapy.org/en/latest/topics/contracts.html\n",
      "https://doc.scrapy.org/en/latest/topics/practices.html\n",
      "https://doc.scrapy.org/en/latest/topics/broad-crawls.html\n",
      "https://doc.scrapy.org/en/latest/topics/firefox.html\n",
      "https://doc.scrapy.org/en/latest/topics/firebug.html\n",
      "https://doc.scrapy.org/en/latest/topics/leaks.html\n",
      "https://doc.scrapy.org/en/latest/topics/media-pipeline.html\n",
      "https://doc.scrapy.org/en/latest/topics/deploy.html\n",
      "https://doc.scrapy.org/en/latest/topics/autothrottle.html\n",
      "https://doc.scrapy.org/en/latest/topics/benchmarking.html\n",
      "https://doc.scrapy.org/en/latest/topics/jobs.html\n",
      "https://doc.scrapy.org/en/latest/topics/architecture.html\n",
      "https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n",
      "https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "https://doc.scrapy.org/en/latest/topics/extensions.html\n",
      "https://doc.scrapy.org/en/latest/topics/api.html\n",
      "https://doc.scrapy.org/en/latest/topics/signals.html\n",
      "https://doc.scrapy.org/en/latest/topics/exporters.html\n",
      "https://doc.scrapy.org/en/latest/news.html\n",
      "https://doc.scrapy.org/en/latest/contributing.html\n",
      "https://doc.scrapy.org/en/latest/versioning.html\n",
      "https://doc.scrapy.org/en/latest/index.html\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/intro/tutorial.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/intro/overview.rst\n",
      "https://www.sitemaps.org/index.html\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/intro/install.rst\n",
      "http://lxml.de/\n",
      "https://pypi.python.org/pypi/parsel\n",
      "https://pypi.python.org/pypi/w3lib\n",
      "https://twistedmatrix.com/\n",
      "https://cryptography.io/\n",
      "https://pypi.python.org/pypi/pyOpenSSL\n",
      "http://lxml.de/installation.html\n",
      "https://cryptography.io/en/latest/installation/\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/intro/examples.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/commands.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/spiders.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/selectors.rst\n",
      "https://www.crummy.com/software/BeautifulSoup/\n",
      "https://docs.python.org/2/library/xml.etree.elementtree.html\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/items.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/loaders.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/shell.rst\n",
      "https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/item-pipeline.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/feed-exports.rst\n",
      "https://github.com/boto/botocore\n",
      "https://github.com/boto/boto\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/request-response.rst\n",
      "https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/link-extractors.rst\n",
      "https://github.com/scrapy/scrapy/blob/master/scrapy/linkextractors/__init__.py\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/settings.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/exceptions.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/logging.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/stats.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/email.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/telnetconsole.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/webservice.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/faq.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/debug.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/contracts.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/practices.rst\n",
      "http://www.googleguide.com/cached_pages.html\n",
      "https://www.torproject.org/\n",
      "https://proxymesh.com/\n",
      "https://scrapoxy.io/\n",
      "https://scrapinghub.com/crawlera\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/broad-crawls.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/firefox.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/firebug.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/leaks.rst\n",
      "http://www.evanjones.ca/python-memory.html\n",
      "http://www.evanjones.ca/python-memory-part2.html\n",
      "http://www.evanjones.ca/python-memory-part3.html\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/media-pipeline.rst\n",
      "https://en.wikipedia.org/wiki/SHA_hash_functions\n",
      "https://en.wikipedia.org/wiki/MD5\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/deploy.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/autothrottle.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/benchmarking.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/jobs.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/architecture.rst\n",
      "https://twistedmatrix.com/documents/current/core/howto/defer-intro.html\n",
      "http://jessenoller.com/2009/02/11/twisted-hello-asynchronous-programming/\n",
      "http://krondo.com/an-introduction-to-asynchronous-programming-and-twisted/\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/downloader-middleware.rst\n",
      "https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html\n",
      "https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html\n",
      "https://pypi.python.org/pypi/leveldb\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/spider-middleware.rst\n",
      "https://docs.python.org/2/library/exceptions.html\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/extensions.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/api.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/signals.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/topics/exporters.rst\n",
      "https://github.com/scrapy/scrapy/blob/1.5.1/docs/news.rst\n",
      "https://github.com/scrapy/scrapy/issues/3281\n",
      "https://github.com/scrapy/scrapy/issues/3166\n",
      "https://github.com/scrapy/scrapy/issues/3096\n",
      "https://github.com/scrapy/scrapy/issues/3092\n",
      "https://github.com/scrapy/scrapy/issues/3263\n",
      "https://github.com/scrapy/scrapy/issues/3058\n",
      "https://github.com/scrapy/scrapy/issues/3059\n",
      "https://github.com/scrapy/scrapy/issues/3089\n",
      "https://github.com/scrapy/scrapy/issues/3123\n",
      "https://github.com/scrapy/scrapy/issues/3127\n",
      "https://github.com/scrapy/scrapy/issues/3189\n",
      "https://github.com/scrapy/scrapy/issues/3224\n",
      "https://github.com/scrapy/scrapy/issues/3280\n",
      "https://github.com/scrapy/scrapy/issues/3279\n",
      "https://github.com/scrapy/scrapy/issues/3201\n",
      "https://github.com/scrapy/scrapy/issues/3260\n",
      "https://github.com/scrapy/scrapy/issues/3284\n",
      "https://github.com/scrapy/scrapy/issues/3298\n",
      "https://github.com/scrapy/scrapy/issues/3294\n",
      "https://github.com/scrapy/scrapy/issues/2983\n",
      "https://github.com/scrapy/scrapy/issues/1343\n",
      "https://github.com/scrapy/scrapy/issues/2851\n",
      "https://github.com/scrapy/scrapy/issues/2785\n",
      "https://github.com/scrapy/scrapy/issues/2654\n",
      "https://github.com/scrapy/scrapy/issues/2923\n",
      "https://github.com/scrapy/scrapy/issues/2883\n",
      "https://github.com/scrapy/scrapy/issues/2812\n",
      "https://github.com/scrapy/scrapy/issues/2844\n",
      "https://github.com/scrapy/scrapy/issues/2857\n",
      "https://github.com/scrapy/scrapy/issues/2743\n",
      "https://github.com/scrapy/scrapy/issues/2755\n",
      "https://github.com/scrapy/scrapy/issues/2831\n",
      "https://github.com/scrapy/scrapy/issues/2921\n",
      "https://github.com/scrapy/scrapy/issues/2927\n",
      "https://github.com/scrapy/scrapy/issues/2250\n",
      "https://github.com/scrapy/scrapy/issues/2793\n",
      "https://github.com/scrapy/scrapy/issues/2935\n",
      "https://github.com/scrapy/scrapy/issues/2990\n",
      "https://github.com/scrapy/scrapy/issues/3050\n",
      "https://github.com/scrapy/scrapy/issues/2213\n",
      "https://github.com/scrapy/scrapy/issues/3048\n",
      "https://github.com/scrapy/scrapy/issues/2811\n",
      "https://github.com/scrapy/scrapy/issues/2848\n",
      "https://github.com/scrapy/scrapy/issues/2766\n",
      "https://github.com/scrapy/scrapy/issues/2849\n",
      "https://github.com/scrapy/scrapy/issues/2862\n",
      "https://github.com/scrapy/scrapy/issues/2876\n",
      "https://github.com/scrapy/scrapy/issues/2853\n",
      "https://github.com/scrapy/scrapy/issues/2756\n",
      "https://github.com/scrapy/scrapy/issues/2762\n",
      "https://github.com/scrapy/scrapy/issues/2978\n",
      "https://github.com/scrapy/scrapy/issues/2982\n",
      "https://github.com/scrapy/scrapy/issues/2958\n",
      "https://github.com/scrapy/scrapy/issues/2759\n",
      "https://github.com/scrapy/scrapy/issues/2781\n",
      "https://github.com/scrapy/scrapy/issues/2828\n",
      "https://github.com/scrapy/scrapy/issues/2837\n",
      "https://github.com/scrapy/scrapy/issues/2884\n",
      "https://github.com/scrapy/scrapy/issues/2924\n",
      "https://github.com/scrapy/scrapy/issues/2826\n",
      "https://github.com/scrapy/scrapy/issues/2791\n",
      "https://github.com/scrapy/scrapy/issues/2764\n",
      "https://github.com/scrapy/scrapy/issues/2763\n",
      "https://github.com/scrapy/scrapy/issues/2866\n",
      "https://github.com/scrapy/scrapy/issues/2922\n",
      "https://github.com/scrapy/scrapy/issues/2374\n",
      "https://github.com/scrapy/scrapy/issues/2999\n",
      "https://github.com/scrapy/scrapy/issues/2964\n",
      "https://github.com/scrapy/scrapy/issues/2976\n",
      "https://github.com/scrapy/scrapy/issues/2989\n",
      "https://github.com/scrapy/scrapy/issues/3019\n",
      "https://github.com/scrapy/scrapy/issues/2537\n",
      "https://github.com/scrapy/scrapy/issues/1941\n",
      "https://github.com/scrapy/scrapy/issues/1982\n",
      "https://github.com/scrapy/scrapy/issues/2539\n",
      "https://github.com/scrapy/scrapy/issues/2187\n",
      "https://github.com/scrapy/scrapy/issues/1829\n",
      "https://github.com/scrapy/scrapy/issues/1728\n",
      "https://github.com/scrapy/scrapy/issues/1495\n",
      "https://github.com/scrapy/scrapy/issues/2526\n"
     ]
    }
   ],
   "source": [
    "urllist=['https://doc.scrapy.org/en/latest/intro/tutorial.html']  #the starting url\n",
    "matrix=np.zeros((200,200)) #the matrix\n",
    "num=0\n",
    "fidx=0\n",
    "while num<200 and fidx<len(urllist):\n",
    "    num=crawl(fidx,urllist[fidx],urllist,matrix)\n",
    "    fidx+=1\n",
    "print 'The first 200 URLs are:'\n",
    "for x in urllist:\n",
    "    print x    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2  Create a Stochastic matrix from its resulting crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(H):\n",
    "    n= len(H)\n",
    "    w = zeros(n)\n",
    "    rho = 1./n * ones(n);\n",
    "    for i in range(n):\n",
    "        if multiply.reduce(H[i]== zeros(n)):\n",
    "            w[i] = 1\n",
    "    newH = H + outer((1./n * w),ones(n))\n",
    " \n",
    "    theta=0.85\n",
    "    G = (theta * newH) + ((1-theta) * outer(1./n * ones(n), ones(n)))\n",
    "    ret=1\n",
    "    j=0\n",
    "    while ret > 0.000001:  #until it converges\n",
    "        rho1 = dot(rho,G)\n",
    "        ret = np.linalg.norm(rho1-rho)  \n",
    "        rho=rho1\n",
    "        j+=1\n",
    "    print 'pagerank converge after %d loops'%(j)\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pagerank converge after 19 loops\n",
      "[0.01169629 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629\n",
      " 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629\n",
      " 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629\n",
      " 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629\n",
      " 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629\n",
      " 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629\n",
      " 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629 0.01169629\n",
      " 0.01169629 0.01169629 0.01169629 0.00312836 0.00312376 0.00312376\n",
      " 0.00309634 0.00329923 0.00309634 0.00309634 0.00309634 0.00309634\n",
      " 0.00309634 0.00309634 0.00309634 0.00312836 0.00312836 0.00312836\n",
      " 0.00311513 0.00311513 0.00311513 0.00312836 0.00312836 0.00312376\n",
      " 0.00312376 0.00312836 0.00311935 0.00311935 0.00311935 0.00312376\n",
      " 0.00353818 0.00312376 0.00312376 0.00312836 0.00312836 0.00312836\n",
      " 0.00312836 0.00312836 0.00312836 0.00312836 0.00312836 0.00312836\n",
      " 0.00312836 0.00310717 0.00310717 0.00310717 0.00310717 0.00310717\n",
      " 0.00310717 0.00312836 0.00312836 0.00312836 0.00311513 0.00311513\n",
      " 0.00311513 0.00311513 0.00311513 0.00311513 0.00311513 0.00312836\n",
      " 0.00312836 0.00312836 0.00312836 0.00311513 0.00311513 0.00311513\n",
      " 0.00311513 0.00311513 0.00311513 0.00311513 0.00311513 0.00312376\n",
      " 0.00312376 0.00312836 0.00312836 0.00312376 0.00312836 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114 0.00299114\n",
      " 0.00299114 0.00299114]\n"
     ]
    }
   ],
   "source": [
    "page_matrix=matrix.copy()\n",
    "for i in range(len(matrix)):\n",
    "    k=sum(matrix[i])\n",
    "    if k>0:\n",
    "        page_matrix[i]/=k\n",
    "        \n",
    "H=pagerank(page_matrix)\n",
    "print H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3 Pass it through the Page Rank algorithm and provide the list of the top 5 page URLs in your sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 page URL:\n",
      "https://doc.scrapy.org/en/latest/intro/tutorial.html\n",
      "https://doc.scrapy.org/en/latest/topics/broad-crawls.html\n",
      "https://doc.scrapy.org/en/latest/topics/firefox.html\n",
      "https://doc.scrapy.org/en/latest/topics/firebug.html\n",
      "https://doc.scrapy.org/en/latest/topics/leaks.html\n"
     ]
    }
   ],
   "source": [
    "v=np.argsort(-H)\n",
    "print 'The top 5 page URL:'\n",
    "for i in range(5):\n",
    "    print urllist[v[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.4 For extra credit use the hits algorithm ( with a connectivity matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits(A):\n",
    "    n= len(A)\n",
    "    Au= dot(transpose(A),A)\n",
    "    Hu = dot(A,transpose(A))\n",
    "    a = ones(n); h = ones(n)    \n",
    "    ret=1\n",
    "    j=0\n",
    "    while ret > 0.000001:  #until it converges\n",
    "        a1 = dot(a,Au)\n",
    "        a1= a1/sum(a1)\n",
    "        h1 = dot(h,Hu)\n",
    "        h1 = h1/ sum(h1)\n",
    "        ret = np.linalg.norm(a1-a)+ np.linalg.norm(h1-h)   \n",
    "        a=a1\n",
    "        h=h1\n",
    "        j+=1\n",
    "    print 'Hits converge after %d loops'%(j)\n",
    "    return a\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits converge after 5 loops\n",
      "[0.02047356 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356\n",
      " 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356\n",
      " 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356\n",
      " 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356\n",
      " 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356\n",
      " 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356\n",
      " 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356 0.02047356\n",
      " 0.02047356 0.02047356 0.02047356 0.00048668 0.00048694 0.00048694\n",
      " 0.00048901 0.00097672 0.00048901 0.00048901 0.00048901 0.00048901\n",
      " 0.00048901 0.00048901 0.00048901 0.00048668 0.00048668 0.00048668\n",
      " 0.00048772 0.00048772 0.00048772 0.00048668 0.00048668 0.00048694\n",
      " 0.00048694 0.00048668 0.0004872  0.0004872  0.0004872  0.00048746\n",
      " 0.00146289 0.00048694 0.00048694 0.00048668 0.00048668 0.00048668\n",
      " 0.00048668 0.00048668 0.00048668 0.00048668 0.00048668 0.00048668\n",
      " 0.00048668 0.00048797 0.00048797 0.00048797 0.00048797 0.00048797\n",
      " 0.00048797 0.00048668 0.00048668 0.00048668 0.00048746 0.00048746\n",
      " 0.00048746 0.00048746 0.00048797 0.00048797 0.00048797 0.00048668\n",
      " 0.00048668 0.00048668 0.00048668 0.00048746 0.00048746 0.00048746\n",
      " 0.00048746 0.00048746 0.00048746 0.00048746 0.00048746 0.00048694\n",
      " 0.00048694 0.00048668 0.00048668 0.00048746 0.00048668 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816 0.00050816\n",
      " 0.00050816 0.00050816]\n"
     ]
    }
   ],
   "source": [
    "H1=hits(matrix)\n",
    "print H1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found that if the data is not that big, the different between pagerank and hits algorithm is small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 top page URL:\n",
      "https://doc.scrapy.org/en/latest/intro/tutorial.html\n",
      "https://doc.scrapy.org/en/latest/topics/broad-crawls.html\n",
      "https://doc.scrapy.org/en/latest/topics/firefox.html\n",
      "https://doc.scrapy.org/en/latest/topics/firebug.html\n",
      "https://doc.scrapy.org/en/latest/topics/leaks.html\n"
     ]
    }
   ],
   "source": [
    "v1=np.argsort(-H1)\n",
    "print 'The 5 top page URL:'\n",
    "for i in range(5):\n",
    "    print urllist[v1[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
